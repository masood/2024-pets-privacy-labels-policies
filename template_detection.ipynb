{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0NymFYgdGBN9",
        "PGAPlQ0mIK7Q",
        "EYu16KfJJJJ7",
        "HMggZO5HJs-t",
        "pXBBLOmRJ0Of"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/masood/2024-pets-privacy-labels-policies/blob/main/template_detection.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "zkWnXNmYO-5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Privacy Policy against Popular Template (Demo)"
      ],
      "metadata": {
        "id": "0NymFYgdGBN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install readabilipy langdetect beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbiwXRZRJ8ue",
        "outputId": "c4150eaf-92bf-4284-f586-a48c8bd4cdf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: readabilipy in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from readabilipy) (1.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from readabilipy) (4.9.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from readabilipy) (2024.5.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->readabilipy) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports needed from pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "#Some built-in imports\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import requests\n",
        "import random\n",
        "import string\n",
        "import json\n",
        "\n",
        "from readabilipy import simple_json_from_html_string\n",
        "from langdetect import detect\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvvGcwTwGATa",
        "outputId": "a142b39f-e0e1-4565-a34c-4d3cc1837831"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Helper/Utitility Functions to Process Text"
      ],
      "metadata": {
        "id": "PGAPlQ0mIK7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/masood/2024-pets-privacy-labels-policies/main/template-files/word2idx_300.pkl\" -O word2idx_300.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hErHD-SoIPnJ",
        "outputId": "b8cf6ab5-25cf-4867-f122-cf03395382b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-29 16:41:46--  https://raw.githubusercontent.com/masood/2024-pets-privacy-labels-policies/main/template-files/word2idx_300.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2064330 (2.0M) [application/octet-stream]\n",
            "Saving to: ‘word2idx_300.pkl’\n",
            "\n",
            "\rword2idx_300.pkl      0%[                    ]       0  --.-KB/s               \rword2idx_300.pkl    100%[===================>]   1.97M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-29 16:41:46 (114 MB/s) - ‘word2idx_300.pkl’ saved [2064330/2064330]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('word2idx_300.pkl', 'rb') as dictionary_file:\n",
        "    dictionary = pickle.load(dictionary_file)"
      ],
      "metadata": {
        "id": "YXL-cgdeIcyM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_serialization(sentence, word2idx, lower_case = True):\n",
        "    \"\"\"\n",
        "\n",
        "    Transforms a sentence into a list of integers. No integer will be appended if the token is not present in word2idx.\n",
        "\n",
        "    Args:\n",
        "        sentence: string, sentence that we want to serialize.\n",
        "        word2idx: dictionary, dictionary with words as keys and indexes as values.\n",
        "        lower_case: boolean, turns all words in the sentence to lower case. Useful if word2idx doesn't support upper case words.\n",
        "    Returns:\n",
        "        s_sentence: list, list containing the indexes of the words present in the sentence.\n",
        "        s_sentence stands for serialized sentence.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    s_sentence = []\n",
        "\n",
        "    not_found = 0\n",
        "\n",
        "    if lower_case:\n",
        "\n",
        "        tokens = map(str.lower,nltk.word_tokenize(sentence))\n",
        "\n",
        "    else:\n",
        "\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "    for token in tokens:\n",
        "\n",
        "        try:\n",
        "\n",
        "            s_sentence.append(word2idx[token])\n",
        "\n",
        "        except KeyError:\n",
        "\n",
        "            not_found += 1\n",
        "\n",
        "    return s_sentence"
      ],
      "metadata": {
        "id": "5G9izPD4IgCz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_string(word2idx, segments, max_template_len=-1):\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    This function process all the privacy policy files and transforms all the segments into lists of integers. It also\n",
        "    transforms all the labels into a list of 0s except in the positions associated with the labels in which we will find 1s\n",
        "    where we will find a 1. It will also place .pkl files into the processed_data folder so that we can load the data from\n",
        "    there instead of having to process the whole dataset.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    Helper functions\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def stack_segments(segments, max_template_len=-1, clearance=2):\n",
        "\n",
        "        segments_len = map(len, segments)\n",
        "\n",
        "        if (max_template_len == -1):\n",
        "            max_len = max(segments_len)\n",
        "            max_template_len = max_len\n",
        "        else:\n",
        "            max_len = max_template_len\n",
        "\n",
        "        segments_list = []\n",
        "\n",
        "        output_len = max_len + clearance * 2\n",
        "\n",
        "        for i, segment in enumerate(segments):\n",
        "            segment_array = np.array(segment)\n",
        "\n",
        "            if (len(segment_array) > max_len):\n",
        "                segment_array = segment_array[:max_len]\n",
        "\n",
        "            zeros_to_prepend = int((output_len - len(segment_array)) / 2)\n",
        "\n",
        "            zeros_to_append = output_len - len(segment_array) - zeros_to_prepend\n",
        "\n",
        "            resized_array = np.append(np.zeros(zeros_to_prepend), segment_array)\n",
        "\n",
        "            resized_array = np.append(resized_array, np.zeros(zeros_to_append))\n",
        "\n",
        "            segments_list.append(torch.tensor(resized_array, dtype=torch.int64))\n",
        "\n",
        "            segments_tensor = torch.stack(segments_list).unsqueeze(1)\n",
        "\n",
        "        return segments_tensor, max_template_len\n",
        "\n",
        "    num_records = len(segments)\n",
        "\n",
        "    segments_matrices = np.zeros(num_records, dtype='object')\n",
        "\n",
        "    for index in range(num_records):\n",
        "        segment = segments[index]\n",
        "        segments_matrices[index] = sentence_serialization(segment, word2idx)\n",
        "\n",
        "    segments_tensor, max_template_len = stack_segments(segments_matrices, max_template_len)\n",
        "\n",
        "    return segments_tensor, max_template_len"
      ],
      "metadata": {
        "id": "x_iTwXO_Im-8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(policy_text):\n",
        "    sentence_list = []\n",
        "    for line in policy_text.splitlines():\n",
        "        sentence_list.extend(sent_tokenize(line))\n",
        "    return sentence_list"
      ],
      "metadata": {
        "id": "qKrJ3ylBIqup"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download and Prepare Template Files"
      ],
      "metadata": {
        "id": "e4T6BY1AGfly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"https://raw.githubusercontent.com/masood/2024-pets-privacy-labels-policies/main/template-files/\"\n",
        "template_files = [\n",
        "    'app-privacy-policy-generator-firebaseapp.txt',\n",
        "    'enzuzo.txt',\n",
        "    'FreePrivacyPolicy.txt',\n",
        "    'getterms.txt',\n",
        "    'iubenda.txt',\n",
        "    'pandadoc.txt',\n",
        "    'PrivacyPolicies.txt',\n",
        "    'PrivacyPolicyGenerator.txt',\n",
        "    'PrivacyPolicyGeneratorInfo.txt',\n",
        "    'securiti-ai.txt',\n",
        "    'shopify.txt',\n",
        "    'Termly.txt',\n",
        "    'TermsFeed.txt',\n",
        "    'website-policies-com.txt',\n",
        "    'WebsitePrivacyPolicyGenerator.txt'\n",
        "]\n",
        "template_sites = [\n",
        "    'https://app-privacy-policy-generator.firebaseapp.com/',\n",
        "    'https://www.enzuzo.com/privacy-policy-generator',\n",
        "    'https://www.freeprivacypolicy.com/free-privacy-policy-generator/',\n",
        "    'https://getterms.io/',\n",
        "    'https://www.iubenda.com/',\n",
        "    'https://www.pandadoc.com/free-privacy-policy-template/',\n",
        "    'https://www.privacypolicies.com/',\n",
        "    'https://www.privacypolicygenerator.org',\n",
        "    'https://www.privacypolicygenarator.info/',\n",
        "    'https://securiti.ai/privacy-center/',\n",
        "    'https://www.shopify.com/tools/policy-generator',\n",
        "    'https://termly.io/products/privacy-policy-generator/',\n",
        "    'https://www.termsfeed.com',\n",
        "    'https://websitepolicies.com',\n",
        "    'https://www.websiteprivacypolicygenerator.com/'\n",
        "]"
      ],
      "metadata": {
        "id": "-9QVJ15GGkgR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_templates = []\n",
        "template_vectors = []\n",
        "max_template_lens = []\n",
        "for template_file in template_files:\n",
        "\n",
        "        # Get Template from File\n",
        "        response = requests.get(base_path + template_file)\n",
        "\n",
        "        # Gather Template as Sentences\n",
        "        processed_template = get_sentences(response.text)\n",
        "\n",
        "        # Vectorize Template\n",
        "        template_vector, max_template_len = vectorize_string(dictionary, processed_template)\n",
        "\n",
        "        processed_templates.append(processed_template)\n",
        "        template_vectors.append(template_vector)\n",
        "        max_template_lens.append(max_template_len)"
      ],
      "metadata": {
        "id": "x-pHiGJSHjg5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Download and Process Privacy Policy"
      ],
      "metadata": {
        "id": "EYu16KfJJJJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions to Clean Extracted Text"
      ],
      "metadata": {
        "id": "HMggZO5HJs-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_out_headings(policy_text, html_content):\n",
        "    def getTextFromTag(html_string, tag):\n",
        "        header_lines = []\n",
        "        soup = BeautifulSoup(html_string, 'html.parser')\n",
        "        for element in soup.find_all(tag):\n",
        "            header_lines.append(element.text)\n",
        "        return header_lines\n",
        "    policy_headings_text = getTextFromTag(html_content, ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "    policy_text_filtered_headers = [x for x in policy_text if x not in policy_headings_text]\n",
        "    return policy_text_filtered_headers\n",
        "\n",
        "def merge_lists(policy_text):\n",
        "    policy_text_filtered_lists = []\n",
        "    for line_index in range(len(policy_text)):\n",
        "        if policy_text[line_index][-1] == ',':\n",
        "            whole_segment = policy_text[line_index].split('*')\n",
        "            avg_len = 0\n",
        "            for list_element in whole_segment:\n",
        "                avg_len += len(list_element.split())\n",
        "            avg_len = avg_len / len(whole_segment)\n",
        "            if (avg_len >= 20):\n",
        "                for list_element in whole_segment:\n",
        "                    policy_text_filtered_lists.append(list_element.strip())\n",
        "            else:\n",
        "                if (len(policy_text_filtered_lists) == 0):\n",
        "                    policy_text_filtered_lists = [policy_text[line_index]]\n",
        "                else:\n",
        "                    policy_text_filtered_lists[-1] += policy_text[line_index]\n",
        "        else:\n",
        "            policy_text_filtered_lists.append(policy_text[line_index])\n",
        "    return policy_text_filtered_lists\n",
        "\n",
        "def remove_short_sentences(policy_text):\n",
        "    policy_text_filtered_lists = []\n",
        "    for line_index in range(len(policy_text)):\n",
        "        num_words = len(policy_text[line_index].split(' '))\n",
        "        if (num_words >= 20):\n",
        "            policy_text_filtered_lists.append(policy_text[line_index].strip())\n",
        "    return policy_text_filtered_lists\n",
        "\n",
        "def find_and_remove_large_string(strings):\n",
        "    def preprocess_string(s):\n",
        "        # Convert to lowercase and remove punctuation\n",
        "        return s.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    def is_substring_found(substring, large_string):\n",
        "        # Check if 90% of the substring is in the large string\n",
        "        substring_length = len(substring)\n",
        "        match_length = int(substring_length * 0.9)\n",
        "\n",
        "        for i in range(len(large_string) - match_length + 1):\n",
        "            if substring[:match_length] in large_string[i:i+match_length]:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def contains_all_substrings(large_string, substrings):\n",
        "        count = 0\n",
        "        for substring in substrings:\n",
        "            if is_substring_found(substring, large_string):\n",
        "                count += 1\n",
        "        return count / len(substrings) >= 0.50\n",
        "\n",
        "    preprocessed_strings = [preprocess_string(s) for s in strings]\n",
        "\n",
        "    for large_string in strings:\n",
        "        preprocessed_large_string = preprocess_string(large_string)\n",
        "\n",
        "        # Check if this string contains 90% of the other preprocessed strings\n",
        "        other_strings = [s for s in preprocessed_strings if s != preprocessed_large_string]\n",
        "        if contains_all_substrings(preprocessed_large_string, other_strings):\n",
        "            strings.remove(large_string)\n",
        "            return strings\n",
        "\n",
        "    return strings"
      ],
      "metadata": {
        "id": "cV2bErZOJQGo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Functions to Visit URL and Collect Policy Text"
      ],
      "metadata": {
        "id": "pXBBLOmRJ0Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_policy_text(privacy_policy_url):\n",
        "    user_agents = [\n",
        "        \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\",\n",
        "        \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1)\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.16; rv:86.0) Gecko/20100101 Firefox/86.0\"\n",
        "    ]\n",
        "\n",
        "    request_headers = {\n",
        "        \"Accept\": \"text/html\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
        "        \"Origin\": \"https://apps.apple.com\",\n",
        "        \"Referer\": \"https://apps.apple.com\",\n",
        "        \"User-Agent\": random.choice(user_agents),\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(privacy_policy_url, headers=request_headers, timeout=15)\n",
        "    except:\n",
        "        print(\"Error with Request\")\n",
        "        return\n",
        "\n",
        "    if response.status_code >= 200 and response.status_code < 400:\n",
        "        try:\n",
        "            html_content = response.text\n",
        "        except:\n",
        "            print(\"Error extracting HTML content\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            article = simple_json_from_html_string(html_content, use_readability=True)\n",
        "        except:\n",
        "            try:\n",
        "                article = simple_json_from_html_string(html_content, use_readability=False)\n",
        "            except:\n",
        "                print(\"Error parsing text from HTML content\")\n",
        "\n",
        "        if 'plain_text' in article and article['plain_text']:\n",
        "            return list(set(list(map(lambda x: x['text'], article['plain_text'])))), article['content']"
      ],
      "metadata": {
        "id": "iV87jFggJ1Qr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Compare Policy to Templates"
      ],
      "metadata": {
        "id": "JTyjh8J2K6Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(policy_text):\n",
        "    # Parse policy text as a list of strings\n",
        "    policy_text = get_sentences(policy_text)\n",
        "\n",
        "    matched_templates = []\n",
        "\n",
        "    for template_index in range(len(template_vectors)):\n",
        "\n",
        "            template_file_name = template_files[template_index]\n",
        "            template_site = template_sites[template_index]\n",
        "            processed_template = processed_templates[template_index]\n",
        "            template_vector = template_vectors[template_index]\n",
        "            max_template_len = max_template_lens[template_index]\n",
        "\n",
        "            try:\n",
        "                # Vectorize Policy Text, Use Max Template Len as cutoff\n",
        "                policy_vector, max_policy_len = vectorize_string(dictionary, policy_text, max_template_len)\n",
        "            except Exception as timeout_exception:\n",
        "                print(f'TIMED OUT! Template: {template_file_name}')\n",
        "                continue\n",
        "\n",
        "\n",
        "            matched_sentences = [0] * len(processed_template)\n",
        "            for policy_sent_index in range(policy_vector.shape[0]):\n",
        "                for template_sent_index in range(template_vector.shape[0]):\n",
        "                    cos = nn.CosineSimilarity(dim=0)\n",
        "                    output = cos(template_vector[template_sent_index][0].float(), policy_vector[policy_sent_index][0].float())\n",
        "                    if (output >= 0.80):\n",
        "                        matched_sentences[template_sent_index] = 1\n",
        "\n",
        "            if (matched_sentences.count(1) / len(matched_sentences)) >= 0.50:\n",
        "                if (matched_sentences.count(1) / policy_vector.shape[0]) >= 0.50:\n",
        "                    matched_templates.append(template_site)\n",
        "    return matched_templates"
      ],
      "metadata": {
        "id": "rtks6vDqK9DM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Putting it all Together"
      ],
      "metadata": {
        "id": "iibqiXQlMYkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "privacy_policy_urls = {\n",
        "    'som': 'https://www.som.org.uk/privacy-and-cookie-policy'\n",
        "}"
      ],
      "metadata": {
        "id": "l4lWX5W8KG0a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_segments, html_content = get_policy_text(privacy_policy_urls['som'])\n",
        "policy_segments = merge_lists(policy_segments)\n",
        "policy_segments = filter_out_headings(policy_segments, html_content)\n",
        "policy_segments = remove_short_sentences(policy_segments)\n",
        "policy_segments = ' '.join(find_and_remove_large_string(policy_segments))"
      ],
      "metadata": {
        "id": "EpNiLpwlKErh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matched_templates = evaluate_policy(policy_segments)\n",
        "\n",
        "if len(matched_templates) > 0:\n",
        "    print('Policy is similar to templates from the follwing sites:')\n",
        "    print(json.dumps(matched_templates))\n",
        "else:\n",
        "    print('Policy did not match any template against which we check.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zugxgqq7KciB",
        "outputId": "700dc459-a2c3-49ab-dad4-2d454d70bbc2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy is similar to templates from the follwing sites:\n",
            "[\"https://www.freeprivacypolicy.com/free-privacy-policy-generator/\", \"https://securiti.ai/privacy-center/\", \"https://termly.io/products/privacy-policy-generator/\"]\n"
          ]
        }
      ]
    }
  ]
}